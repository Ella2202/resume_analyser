{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04de14fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfplumber in c:\\users\\afroze\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.11.7)\n",
      "Requirement already satisfied: pytesseract in c:\\users\\afroze\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.3.13)\n",
      "Requirement already satisfied: pdf2image in c:\\users\\afroze\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.17.0)\n",
      "Requirement already satisfied: pdfminer.six==20250506 in c:\\users\\afroze\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pdfplumber) (20250506)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\users\\afroze\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pdfplumber) (10.4.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\afroze\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pdfplumber) (4.30.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\afroze\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pdfminer.six==20250506->pdfplumber) (3.3.2)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\afroze\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pdfminer.six==20250506->pdfplumber) (45.0.6)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\afroze\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytesseract) (24.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\afroze\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.16.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\afroze\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cffi>=1.14->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pdfplumber pytesseract pdf2image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9271e01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70189037",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        # Try direct text extraction\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text\n",
    "\n",
    "        if text.strip():\n",
    "            return text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Direct text extraction failed: {e}\")\n",
    "\n",
    "    # Fallback to OCR for image-based PDFs\n",
    "    print(\"Falling back to OCR for image-based PDF.\")\n",
    "    try:\n",
    "        images = convert_from_path(pdf_path)\n",
    "        for image in images:\n",
    "            page_text = pytesseract.image_to_string(image)\n",
    "            text += page_text + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"OCR failed: {e}\")\n",
    "\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3248e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Text from PDF:\n",
      "AFROZE NAZIRA | BS21B003 |LinkedIn| GitHub\n",
      "EDUCATION AND SCHOLASTIC ACHIEVEMENTS\n",
      "Program Institute % / CGPA Year of Completion\n",
      "Dual Degree in Biological Sciences Indian Institute of Technology, Madras 7.85 2026\n",
      "Class XII Rosary Matriculation Higher Secondary School 85% 2020\n",
      "Class X Rosary Matriculation Higher Secondary School 95% 2018\n",
      "Scholastic Achievements Achieved 99.1 percentile in Tamil Nadu SSLC, and honored with the prestigious Tamgrads Scholarship. (2018)\n",
      "COURSEWORK & SKILLSET\n",
      "● Reinforcement\n",
      "● Machine Learning Techniques ● Python for beginners ● Bioinformatics ● Computer Simulations\n",
      "Learning\n",
      "● Deep Learning ● Programming in C++ ● Biostatistics ● Principles of Economics\n",
      "● Recommender Systems\n",
      "Programming Languages: Python, C, C++, Java, SQL Tools: Excel,numpy, pandas, tensorflow, scikit-learn, Jupyter, PyTorch, Git\n",
      "RESEARCH EXPERIENCE\n",
      "Walmart Centre for Joined as a Project Associate under Prof. Rakhi Singh, leading MSME Survey & Data Analysis for Policy\n",
      "Tech Excellence Recommendation\n",
      "Project Associate ● Analyzed previous surveys and research papers on Discrete Choice Experiments for survey design and data analysis\n",
      "● Will process the raw data and apply machine learning techniques to analyze it, generating data-driven insights\n",
      "Ashok Leyland Reduced the probability of road accidents by developing driver intoxication detection technology in steering wheel\n",
      "Product Development ● Used SVM algorithm to analyze PPG sensor data, predicting driver intoxication with an impressive accuracy of 95%\n",
      "(June’23 -July’23) ● Achieved a 30% increase in the sensitivity of the sensor with multiple sensor integration & continuous monitoring.\n",
      "ISMO Biophotonics Evaluated the potential efficacy of microfluidic chips to culture cancer cells to test drugs for personalized treatment.\n",
      "Industrial Intern ● Conducted research on 2D & 3D cell culture models to analyze the feasibility in simulating tumor microenvironment\n",
      "(Dec’22 - Jan’23) ● Explored 10+ imaging and analytical techniques to assess the characteristics & behavior of cancer cells.\n",
      "PROJECTS\n",
      "● Executed sentiment analysis on Amazon reviews utilizing NLTK for initial text processing and sentiment analysis with\n",
      "Sentimental Analysis VADER, and transformer-based model (RoBERTa) provided by Hugging Face enhancing model accuracy.\n",
      "● Evaluated model performances, highlighting the effectiveness of transformer models over traditional methods.\n",
      "● Built a neural network for customer churn prediction, achieving 80% accuracy in identifying attrition patterns.\n",
      "Customer Churn ● Implemented feature engineering techniques and normalization processes to enhance the model’s efficiency, such as\n",
      "Prediction one-hot encoding of categorical variables and normalization of continuous variables using Min-Max scaling.\n",
      "● Compiled the model with Binary cross entropy as the loss function and Adam optimizer for updating weights.\n",
      "● Developed a data preprocessing pipeline for the Wisconsin Diagnostic Dataset to enhance model performance.\n",
      "Breast Cancer ● Implemented machine learning algorithms including Logistic Regression, Random Forest, XGBoost, Gradient\n",
      "Prediction Boosting, & Naive Bayes, achieving a maximum accuracy of 96%, which helps enhance clinical decision making.\n",
      "● Evaluated model performance by producing detailed metrics such as confusion matrices and classification reports.\n",
      "Customer ● Developed a customer segmentation model facilitating targeted marketing and personalized customer service.\n",
      "Segmentation ● Optimized K-means clustering for classification by income and spending, using elbow method, for optimal clusters.\n",
      "PROFESSIONAL EXPERIENCE\n",
      "Procter & Gamble Among the top 50 candidates, out of 7k+ Applicants across India, selected for the P&G Spotlight’24: Product Supply.\n",
      "Product Supply ● Optimized the working of a 12-step gamified chain across a period of 3 days as part of the National Level program.\n",
      "(May’24) ● Examined the Pampers & Tide plants; Analyzed the assembly line efficiency identifying key areas for improvement.\n",
      "Apollo Hospitals Led a team of 4 interns to manage the marketing and public relations endeavors for Apollo hospitals for 4 months.\n",
      "Management Trainee ● Leveraged social media analytics and AI tools to improve SEO and SMO, ensuring data-driven decision-making.\n",
      "(June’24 - Sept’24) ● Secured coverage in 15+ media outlets by strategizing marketing initiatives for 2 International Conferences at Apollo.\n",
      "POSITIONS OF RESPONSIBILITY\n",
      "● Facilitated placements and internships of 1500+ IITM students in 50+ business, finance, analytics, and tech firms.\n",
      "IPR Coordinator\n",
      "● Led outreach to 100+ banks & achieved a 30% inc in conversions by improving the corporate mailing strategy.\n",
      "(May’23 -Mar’24)\n",
      "● Spearheaded the 1st batch of Auxiliary Coordinators to expand outreach with new innovative mediums effectively.\n",
      "Dept Coordinator ● Achieved a 43% increase in the number of offers provided to the final and pre-final year students of Biotechnology.\n",
      "(Apr’22 - Mar’23) ● Guided 25+ final-year students applying for placements and pre-final-year students for internships throughout.\n",
      "Saarang-FinanceCoor ● Executed an important role in structuring the budget of ~INR1.9 crores for Saarang 2023, with over 70k+ in footfall.\n",
      "d ● Managed the top 31 restaurateurs of Chennai for sponsorship deals, generating over INR16.5 Lakhs in revenue.\n",
      "(Apr’22 - May’23)\n",
      "EXTRACURRICULARS & CO-CURRICULAR\n",
      "Hackathon ● Placed first among 40+ participants in the ‘Spot the Crack’ hackathon conducted by Artificial Intelligence club, CFI*\n",
      "Choreo ● Placed first among 15+ teams in the inter-hostel Competition “Loop Dance” at Litsoc 2021-22, leading a team of 10.Millenium Fellow ● Headed the EmpowerVillage project, facilitating the empowerment of 20+ rural women living in Chennai's outskirts.\n"
     ]
    }
   ],
   "source": [
    "pdf_path = \"Afroze_ML.pdf\"\n",
    "resume_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "print(\"\\nExtracted Text from PDF:\")\n",
    "print(resume_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6415820",
   "metadata": {},
   "source": [
    "Set Google GenerativeAI API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6706a9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Collecting google.generativeai\n",
      "  Downloading google_generativeai-0.8.5-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting google-ai-generativelanguage==0.6.15 (from google.generativeai)\n",
      "  Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting google-api-core (from google.generativeai)\n",
      "  Downloading google_api_core-2.25.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting google-api-python-client (from google.generativeai)\n",
      "  Downloading google_api_python_client-2.179.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting google-auth>=2.15.0 (from google.generativeai)\n",
      "  Downloading google_auth-2.40.3-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: protobuf in c:\\users\\afroze\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google.generativeai) (5.29.5)\n",
      "Requirement already satisfied: pydantic in c:\\users\\afroze\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google.generativeai) (2.11.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\afroze\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google.generativeai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\afroze\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google.generativeai) (4.13.2)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-ai-generativelanguage==0.6.15->google.generativeai)\n",
      "  Downloading proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core->google.generativeai)\n",
      "  Downloading googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in c:\\users\\afroze\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-core->google.generativeai) (2.31.0)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=2.15.0->google.generativeai)\n",
      "  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=2.15.0->google.generativeai)\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=2.15.0->google.generativeai)\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting httplib2<1.0.0,>=0.19.0 (from google-api-python-client->google.generativeai)\n",
      "  Downloading httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client->google.generativeai)\n",
      "  Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client->google.generativeai)\n",
      "  Downloading uritemplate-4.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\afroze\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic->google.generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in c:\\users\\afroze\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic->google.generativeai) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\afroze\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic->google.generativeai) (0.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\afroze\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm->google.generativeai) (0.4.6)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in c:\\users\\afroze\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google.generativeai) (1.74.0)\n",
      "Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google.generativeai)\n",
      "  Downloading grpcio_status-1.74.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\afroze\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google.generativeai) (3.2.0)\n",
      "Collecting pyasn1<0.7.0,>=0.6.1 (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google.generativeai)\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\afroze\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google.generativeai) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\afroze\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google.generativeai) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\afroze\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google.generativeai) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\afroze\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google.generativeai) (2024.2.2)\n",
      "INFO: pip is looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google.generativeai)\n",
      "  Downloading grpcio_status-1.73.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.73.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.72.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.72.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.71.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Downloading google_generativeai-0.8.5-py3-none-any.whl (155 kB)\n",
      "   ---------------------------------------- 0.0/155.4 kB ? eta -:--:--\n",
      "   ------------------------------- -------- 122.9/155.4 kB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 155.4/155.4 kB 2.3 MB/s eta 0:00:00\n",
      "Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.1/1.3 MB 2.9 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 0.2/1.3 MB 2.5 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 0.3/1.3 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 0.4/1.3 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 0.5/1.3 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 0.7/1.3 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 0.7/1.3 MB 2.3 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 0.9/1.3 MB 2.4 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.0/1.3 MB 2.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.1/1.3 MB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.2/1.3 MB 2.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.3/1.3 MB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.3/1.3 MB 2.2 MB/s eta 0:00:00\n",
      "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Downloading google_api_core-2.25.1-py3-none-any.whl (160 kB)\n",
      "   ---------------------------------------- 0.0/160.8 kB ? eta -:--:--\n",
      "   --------------- ------------------------ 61.4/160.8 kB 1.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 153.6/160.8 kB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 160.8/160.8 kB 1.4 MB/s eta 0:00:00\n",
      "Downloading google_auth-2.40.3-py2.py3-none-any.whl (216 kB)\n",
      "   ---------------------------------------- 0.0/216.1 kB ? eta -:--:--\n",
      "   -------------------- ------------------- 112.6/216.1 kB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 216.1/216.1 kB 2.6 MB/s eta 0:00:00\n",
      "Downloading google_api_python_client-2.179.0-py3-none-any.whl (14.0 MB)\n",
      "   ---------------------------------------- 0.0/14.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/14.0 MB 1.8 MB/s eta 0:00:08\n",
      "    --------------------------------------- 0.3/14.0 MB 2.6 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.4/14.0 MB 2.6 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.5/14.0 MB 2.6 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.6/14.0 MB 2.5 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 0.8/14.0 MB 2.5 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 0.9/14.0 MB 2.5 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 1.0/14.0 MB 2.5 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 1.1/14.0 MB 2.4 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 1.2/14.0 MB 2.3 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 1.3/14.0 MB 2.3 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 1.4/14.0 MB 2.3 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 1.5/14.0 MB 2.3 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 1.6/14.0 MB 2.3 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 1.7/14.0 MB 2.2 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 1.8/14.0 MB 2.2 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 1.9/14.0 MB 2.2 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 2.0/14.0 MB 2.2 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 2.1/14.0 MB 2.2 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 2.2/14.0 MB 2.2 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 2.3/14.0 MB 2.2 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 2.4/14.0 MB 2.2 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 2.5/14.0 MB 2.2 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 2.6/14.0 MB 2.2 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 2.7/14.0 MB 2.2 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 2.8/14.0 MB 2.2 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 2.9/14.0 MB 2.2 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 3.0/14.0 MB 2.2 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 3.1/14.0 MB 2.2 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 3.2/14.0 MB 2.2 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 3.3/14.0 MB 2.2 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 3.4/14.0 MB 2.2 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 3.5/14.0 MB 2.2 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 3.6/14.0 MB 2.2 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 3.7/14.0 MB 2.2 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 3.8/14.0 MB 2.2 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 3.9/14.0 MB 2.2 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 4.0/14.0 MB 2.2 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 4.1/14.0 MB 2.2 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 4.2/14.0 MB 2.2 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 4.3/14.0 MB 2.1 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 4.4/14.0 MB 2.2 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 4.7/14.0 MB 2.2 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 4.8/14.0 MB 2.2 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 4.9/14.0 MB 2.2 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 4.9/14.0 MB 2.2 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 5.1/14.0 MB 2.2 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 5.2/14.0 MB 2.2 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 5.3/14.0 MB 2.2 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 5.4/14.0 MB 2.2 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 5.5/14.0 MB 2.2 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 5.6/14.0 MB 2.2 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 5.7/14.0 MB 2.2 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 5.8/14.0 MB 2.2 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 5.9/14.0 MB 2.2 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 6.1/14.0 MB 2.2 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 6.1/14.0 MB 2.2 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 6.3/14.0 MB 2.2 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 6.4/14.0 MB 2.2 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 6.5/14.0 MB 2.2 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 6.6/14.0 MB 2.2 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 6.7/14.0 MB 2.2 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 6.9/14.0 MB 2.2 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 7.0/14.0 MB 2.2 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 7.1/14.0 MB 2.2 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 7.3/14.0 MB 2.2 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 7.4/14.0 MB 2.2 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 7.5/14.0 MB 2.3 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 7.6/14.0 MB 2.3 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 7.8/14.0 MB 2.3 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 7.9/14.0 MB 2.3 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 8.0/14.0 MB 2.3 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 8.0/14.0 MB 2.2 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 8.2/14.0 MB 2.3 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 8.3/14.0 MB 2.3 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 8.4/14.0 MB 2.2 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 8.5/14.0 MB 2.3 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 8.6/14.0 MB 2.2 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 8.7/14.0 MB 2.3 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 8.8/14.0 MB 2.2 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 8.9/14.0 MB 2.2 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 9.0/14.0 MB 2.3 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 9.2/14.0 MB 2.3 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 9.3/14.0 MB 2.2 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 9.4/14.0 MB 2.3 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 9.5/14.0 MB 2.2 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 9.6/14.0 MB 2.3 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 9.7/14.0 MB 2.3 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 9.8/14.0 MB 2.3 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 9.9/14.0 MB 2.3 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 10.0/14.0 MB 2.3 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 10.1/14.0 MB 2.2 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 10.1/14.0 MB 2.2 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 10.2/14.0 MB 2.2 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 10.3/14.0 MB 2.2 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 10.4/14.0 MB 2.2 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 10.5/14.0 MB 2.2 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 10.6/14.0 MB 2.2 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 10.7/14.0 MB 2.2 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 10.7/14.0 MB 2.2 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 10.8/14.0 MB 2.2 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 10.9/14.0 MB 2.2 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 11.0/14.0 MB 2.2 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 11.1/14.0 MB 2.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 11.2/14.0 MB 2.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 11.3/14.0 MB 2.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 11.4/14.0 MB 2.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 11.5/14.0 MB 2.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 11.6/14.0 MB 2.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 11.7/14.0 MB 2.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 11.8/14.0 MB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 11.9/14.0 MB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 12.0/14.0 MB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 12.1/14.0 MB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 12.2/14.0 MB 2.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 12.3/14.0 MB 2.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 12.3/14.0 MB 2.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 12.4/14.0 MB 2.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 12.4/14.0 MB 2.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 12.5/14.0 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 12.6/14.0 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 12.6/14.0 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 12.7/14.0 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 12.7/14.0 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 12.8/14.0 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 13.0/14.0 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 13.1/14.0 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 13.1/14.0 MB 2.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 13.3/14.0 MB 2.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 13.4/14.0 MB 2.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 13.4/14.0 MB 2.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 13.5/14.0 MB 2.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 13.6/14.0 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.7/14.0 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.7/14.0 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.8/14.0 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.9/14.0 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.9/14.0 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.0/14.0 MB 2.0 MB/s eta 0:00:00\n",
      "Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Downloading googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "   ---------------------------------------- 0.0/294.5 kB ? eta -:--:--\n",
      "   --------------- ------------------------ 112.6/294.5 kB 2.2 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 174.1/294.5 kB 2.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 286.7/294.5 kB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 294.5/294.5 kB 1.8 MB/s eta 0:00:00\n",
      "Downloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "   ---------------------------------------- 0.0/96.9 kB ? eta -:--:--\n",
      "   -------------------------------------- - 92.2/96.9 kB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 96.9/96.9 kB 1.8 MB/s eta 0:00:00\n",
      "Downloading proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.2/50.2 kB 2.7 MB/s eta 0:00:00\n",
      "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "   ---------------------------------------- 0.0/181.3 kB ? eta -:--:--\n",
      "   ------------------------ --------------- 112.6/181.3 kB 3.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 153.6/181.3 kB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 181.3/181.3 kB 1.6 MB/s eta 0:00:00\n",
      "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Downloading uritemplate-4.2.0-py3-none-any.whl (11 kB)\n",
      "Downloading grpcio_status-1.71.2-py3-none-any.whl (14 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "   ---------------------------------------- 0.0/83.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 83.1/83.1 kB 2.4 MB/s eta 0:00:00\n",
      "Installing collected packages: uritemplate, python-dotenv, pyasn1, proto-plus, httplib2, googleapis-common-protos, cachetools, rsa, pyasn1-modules, grpcio-status, google-auth, google-auth-httplib2, google-api-core, google-api-python-client, google-ai-generativelanguage, google.generativeai\n",
      "Successfully installed cachetools-5.5.2 google-ai-generativelanguage-0.6.15 google-api-core-2.25.1 google-api-python-client-2.179.0 google-auth-2.40.3 google-auth-httplib2-0.2.0 google.generativeai-0.8.5 googleapis-common-protos-1.70.0 grpcio-status-1.71.2 httplib2-0.22.0 proto-plus-1.26.1 pyasn1-0.6.1 pyasn1-modules-0.4.2 python-dotenv-1.1.1 rsa-4.9.1 uritemplate-4.2.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install google.generativeai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43c519a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "072e5442",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.generate_content(\"What is the capital of India?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31cdb63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response:\n",
      "GenerateContentResponse(\n",
      "    done=True,\n",
      "    iterator=None,\n",
      "    result=protos.GenerateContentResponse({\n",
      "      \"candidates\": [\n",
      "        {\n",
      "          \"content\": {\n",
      "            \"parts\": [\n",
      "              {\n",
      "                \"text\": \"The capital of India is **New Delhi**.\\n\"\n",
      "              }\n",
      "            ],\n",
      "            \"role\": \"model\"\n",
      "          },\n",
      "          \"finish_reason\": \"STOP\",\n",
      "          \"avg_logprobs\": -0.0009060123004019261\n",
      "        }\n",
      "      ],\n",
      "      \"usage_metadata\": {\n",
      "        \"prompt_token_count\": 7,\n",
      "        \"candidates_token_count\": 10,\n",
      "        \"total_token_count\": 17\n",
      "      },\n",
      "      \"model_version\": \"gemini-1.5-flash\"\n",
      "    }),\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7f8ee02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of India is **New Delhi**.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401f9343",
   "metadata": {},
   "source": [
    "Analysing Resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61968ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_resume(resume_text, job_description=None):\n",
    "    if not resume_text:\n",
    "        return {\"error\": \"Resume text is required for analysis.\"}\n",
    "    \n",
    "    model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "    \n",
    "    base_prompt = f\"\"\"\n",
    "    You are an experienced HR with Technical Experience in the field of any one job role from Data Science, Data Analyst, DevOPS, Machine Learning Engineer, Prompt Engineer, AI Engineer, Full Stack Web Development, Big Data Engineering, Marketing Analyst, Human Resource Manager, Software Developer your task is to review the provided resume.\n",
    "    Please share your professional evaluation on whether the candidate's profile aligns with the role. Also give the ATS score of the resume. ALso mention Skills he already have and siggest some skills to imorve his resume , alos suggest some course he might take to improve the skills.Highlight the strengths and weaknesses.\n",
    "\n",
    "    Resume:\n",
    "    {resume_text}\n",
    "    \"\"\"\n",
    "\n",
    "    if job_description:\n",
    "        base_prompt += f\"\"\"\n",
    "        Additionally, compare this resume to the following job description:\n",
    "        \n",
    "        Job Description:\n",
    "        {job_description}\n",
    "        \n",
    "        Highlight the strengths and weaknesses of the applicant in relation to the specified job requirements.\n",
    "        \"\"\"\n",
    "\n",
    "    response = model.generate_content(base_prompt)\n",
    "\n",
    "    analysis = response.text.strip()\n",
    "    return analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0472e931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Resume Evaluation for Afroze Nazira\n",
      "\n",
      "**Role Assumed for Evaluation:** Data Scientist (This resume shows a strong leaning toward data science roles, but other technical roles are possible depending on the specific requirements.)\n",
      "\n",
      "**Overall Assessment:**\n",
      "\n",
      "Afroze Nazira's resume demonstrates a strong foundation in data science and a commitment to learning.  The resume is well-structured and highlights impressive achievements, particularly in research and extracurricular activities. However, it needs refinement to better target data science positions and showcase skills more directly relevant to industry expectations.\n",
      "\n",
      "\n",
      "**Strengths:**\n",
      "\n",
      "* **Diverse Experience:** The resume showcases a range of experiences including research projects, internships in diverse sectors (healthcare, manufacturing), hackathon participation, and leadership roles.  This demonstrates versatility and adaptability.\n",
      "* **Technical Skills:**  A good range of programming languages (Python, C, C++, Java, SQL) and data science tools (NumPy, Pandas, TensorFlow, Scikit-learn, PyTorch, Jupyter) are listed.  The projects demonstrate practical application of these skills.\n",
      "* **Project Highlights:** The projects (sentiment analysis, customer churn prediction, breast cancer prediction, customer segmentation) are well-described and quantify results (e.g., 96% accuracy in breast cancer prediction).  This is crucial for demonstrating competence.\n",
      "* **Quantifiable Achievements:**  The resume consistently uses numbers to demonstrate the impact of the candidate's contributions (e.g., 30% increase in sensor sensitivity, 30% increase in conversions, INR 16.5 Lakhs in revenue generated). This makes the accomplishments more impactful.\n",
      "* **Leadership Experience:**  The numerous leadership roles demonstrate organizational, interpersonal, and management skills, highly valued in many data science roles.\n",
      "\n",
      "**Weaknesses:**\n",
      "\n",
      "* **Lack of Focus:** The resume tries to showcase everything, potentially diluting the impact of the strongest achievements.  It needs to be more focused on highlighting data science skills and experiences relevant to a target role.\n",
      "* **Inconsistent Detail:**  While project descriptions are detailed, some experiences (e.g., P&G Spotlight) lack specific details on accomplishments and impact.  Quantifiable results should be included for all experiences.\n",
      "* **Skill Gaps:** While possessing a good foundation, some essential data science skills might be missing or under-represented.  For instance, there's minimal mention of data visualization (e.g., Matplotlib, Seaborn), database management (beyond basic SQL), and cloud computing (AWS, Azure, GCP).\n",
      "* **Resume Length:** The resume is quite long. It could be made more concise and impactful by prioritizing the most relevant information and using stronger action verbs.\n",
      "* **Missing Portfolio Link:**  While GitHub and LinkedIn are mentioned, a direct link to a portfolio showcasing projects would significantly strengthen the application.\n",
      "\n",
      "**Skills Afroze Already Has:**\n",
      "\n",
      "* Python, C, C++, Java, SQL\n",
      "* NumPy, Pandas, TensorFlow, Scikit-learn, PyTorch, Jupyter, Git\n",
      "* Machine learning techniques (SVM, Logistic Regression, Random Forest, XGBoost, Gradient Boosting, Naive Bayes)\n",
      "* Deep learning (mentioned as learning, but needs more demonstration)\n",
      "* Data preprocessing, feature engineering, model evaluation\n",
      "* Data analysis, statistical analysis (Biostatistics mentioned)\n",
      "* Sentiment analysis, customer churn prediction, customer segmentation\n",
      "* Data visualization (implied, but needs explicit mention and examples)\n",
      "\n",
      "\n",
      "**Skills to Improve:**\n",
      "\n",
      "* **Advanced Machine Learning Techniques:**  Explore and demonstrate proficiency in more advanced techniques like neural networks (beyond basic implementation), time series analysis, natural language processing (beyond basic sentiment analysis), and unsupervised learning methods.\n",
      "* **Big Data Technologies:** Gain experience with tools like Spark, Hadoop, or cloud-based big data solutions.\n",
      "* **Data Visualization:** Develop strong data visualization skills using libraries like Matplotlib and Seaborn.  Include examples in a portfolio.\n",
      "* **Cloud Computing:** Familiarize oneself with at least one major cloud platform (AWS, Azure, or GCP).\n",
      "* **Database Management:** Demonstrate expertise beyond basic SQL, including database design and optimization.\n",
      "* **Version Control (Git):**  Show more concrete examples of Git usage in projects.\n",
      "* **Communication Skills:**  Practice clearly and concisely communicating technical concepts in writing and verbally.\n",
      "\n",
      "\n",
      "**Suggested Courses:**\n",
      "\n",
      "* **Advanced Machine Learning Specialization (Coursera):** Offered by various universities, this covers many advanced techniques.\n",
      "* **Deep Learning Specialization (Coursera):**  Focuses on building and training deep neural networks.\n",
      "* **Big Data Specialization (Coursera/edX):**  Introduces tools like Spark and Hadoop.\n",
      "* **Cloud Computing Fundamentals (AWS/Azure/GCP):**  Provides a foundational understanding of cloud platforms.\n",
      "* **Data Visualization with Matplotlib and Seaborn (DataCamp/Udemy):** Develops data visualization expertise.\n",
      "* **SQL for Data Science (DataCamp/Udemy):**  Covers advanced SQL techniques for data manipulation and analysis.\n",
      "\n",
      "\n",
      "**Recommendations:**\n",
      "\n",
      "1. **Refocus the Resume:** Tailor the resume to specifically target data science roles, highlighting relevant skills and experiences.  Reduce the length by prioritizing the most relevant information.\n",
      "2. **Develop a Portfolio:** Create a portfolio website or GitHub repository showcasing completed projects, including code, documentation, and results.\n",
      "3. **Quantify Accomplishments:** Add specific numbers and metrics to all experiences to demonstrate impact.\n",
      "4. **Expand on Technical Skills:**  Explicitly mention and showcase proficiency in the suggested skills to improve.\n",
      "5. **Add a Summary/Objective:** Include a brief summary highlighting key skills and career goals.\n",
      "6. **Proofread Carefully:** Ensure the resume is free of grammatical errors and typos.\n",
      "\n",
      "\n",
      "By addressing these weaknesses and strengthening the highlighted areas, Afroze Nazira can significantly enhance her resume and increase her chances of securing a data science position.\n"
     ]
    }
   ],
   "source": [
    "print(analyze_resume(resume_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c1d049",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
